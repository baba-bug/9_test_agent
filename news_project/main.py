import asyncio
import sys
import json
import os
import hashlib

# Add the current directory to sys.path to allow imports
# å½“æˆ‘ä»¬è¿è¡Œ python news_project/main.py æ—¶ï¼Œæˆ‘ä»¬éœ€è¦è®© Python çŸ¥é“å½“å‰ç›®å½•æ˜¯åŒ…çš„ä¸€éƒ¨åˆ†
# æˆ–è€…ç®€å•åœ°æŠŠå½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•åŠ å…¥ path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from scraper.config import TARGET_URLS
from scraper.core import fetch_webpage, extract_news_with_ai
from scraper.storage import Storage
from scraper.utils import clean_html_for_ai 
from scraper.rankings import get_venue_score # Added import

async def monitor_news():
    """æ ¸å¿ƒç›‘æ§é€»è¾‘ (Async)"""
    print("=" * 60)
    print("ğŸŒ AI Multi-Site News Monitor (Optimized)")
    print("=" * 60)
    
    # åˆå§‹åŒ–å­˜å‚¨
    storage = Storage()
    all_new_articles = []
    
    for url in TARGET_URLS:
        html = await fetch_webpage(url)
        if html:
            # 1. è®¡ç®—å†…å®¹æŒ‡çº¹ (MD5)
            # ä¸ºäº†ç¡®ä¿å‡†ç¡®æ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨ clean_html_for_ai å¤„ç†åçš„æ–‡æœ¬è¿›è¡Œ Hash
            # è¿™æ ·å¯ä»¥å¿½ç•¥éå†…å®¹çš„å˜åŠ¨ï¼ˆå¦‚å¹¿å‘ŠIDå˜åŒ–ã€æ—¶é—´æˆ³ç­‰ï¼‰
            cleaned_text = clean_html_for_ai(html, url)
            if not cleaned_text:
                print(f"âš  Empty content from {url}")
                continue
                
            content_hash = hashlib.md5(cleaned_text.encode('utf-8')).hexdigest()
            stored_hash = storage.get_page_hash(url)
            
            # 2. å¯¹æ¯” Hash
            if content_hash == stored_hash:
                print(f"â© [Skipped] Content unchanged for {url}")
                print(f"   (Hash: {content_hash[:8]}...)")
                continue
            
            print(f"ğŸ“ Content changed or new. Processing {url}...")
            
            # 4. åˆ¤æ–­ç±»å‹ (News vs Paper) å¹¶è°ƒç”¨ AI
            mode = "news"
            # Simple heuristic for Paper/Research
            if any(ky in url for ky in ["arxiv.org", ".edu", "publication", "research", "deepmind"]):
                mode = "paper"
            if "openai.com/index" in url: # OpenAI blog often technical but mix.
                mode = "news" # Keep OpenAI as news/product unless strictly research
            
            # extract_news_with_ai å†…éƒ¨ä¼šé‡æ–° cleaningï¼Œæˆ‘ä»¬ä¼  mode è¿›å»
            articles = await extract_news_with_ai(html, url, mode=mode)
            
            # æ›´æ–° Hash (æ— è®ºæ˜¯å¦æå–åˆ°æ–‡ç« ï¼Œåªè¦å†…å®¹å˜äº†å°±æ›´æ–°ï¼Œé¿å…é‡å¤å°è¯•)
            storage.save_page_hash(url, content_hash)
            
            if articles:
                # è¿‡æ»¤æ–°æ–‡ç« 
                new_articles = storage.filter_new_articles(articles)
                
                if new_articles:
                    print(f"âœ¨ Found {len(new_articles)} NEW articles from {url} [{mode.upper()}]")
                    # æ ‡è®°ç±»å‹
                    for art in new_articles:
                        art['type'] = mode 
                    
                    all_new_articles.extend(new_articles)
                    
                    # æ›´æ–°çŠ¶æ€
                    for art in new_articles:
                        storage.add_seen(art['link'])
                else:
                    print(f"ğŸ’¤ No new articles from {url} (found {len(articles)} old ones)")
            else:
                print(f"âš  No articles found from {url}")
        print("-" * 40)
        
    # --- æ’åºé€»è¾‘ (Sorting) ---
    def calculate_final_score(article):
        try:
            # 1. Semantic Score (0-100) - AI's relevance judgment
            semantic = int(article.get('ai_score', 0))
            
            # 2. Impact Score (0-10) - Academic/Industry status
            # Weight x 2 (Max +20 for CCF A / Major Release)
            # User wanted Nature=20, CCF A=10. 
            # If AI returns 10 for CCF A, then x1 is 10. x2 is 20.
            # Let's use x2 to make Impact very visible.
            impact = int(article.get('impact_score', 0)) * 2
            
            # 3. Tech Release Boost (+20)
            tech_boost = 20 if article.get('is_tech_release') else 0
            
            return semantic + impact + tech_boost
        except:
            return 0

    # Sort all new articles
    for art in all_new_articles:
        art['score'] = calculate_final_score(art)
        # AI returns 'score_reason', use it.
        if 'score_reason' not in art:
             art['score_reason'] = "AI scoring unavailable"
        
    all_new_articles.sort(key=lambda x: x['score'], reverse=True)

    # Split into two lists for display
    news_list = [a for a in all_new_articles if a.get('type') == 'news']
    paper_list = [a for a in all_new_articles if a.get('type') == 'paper']

    # å¤„ç†ç»“æœ
    result_message = "No new articles found."
    
    if all_new_articles:
        result_message = f"Found {len(all_new_articles)} new articles."
        print(f"\nğŸ‰ {result_message}")
        
        if not os.getenv("NEWS_BUCKET_NAME"): # æœ¬åœ°æ¨¡å¼æ‰å†™æ–‡ä»¶
            output_file = "latest_new_articles.json"
            history_file = "history_news.json"
            
            # 1. ä¿å­˜æœ¬æ¬¡æ–°æ–‡ç«  (Sorted with Score)
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(all_new_articles, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ Saved latest articles to {output_file}")
            
            # 2. è¿½åŠ åˆ°å†å²æ¡£æ¡ˆ (History Persistence)
            history_data = []
            if os.path.exists(history_file):
                try:
                    with open(history_file, "r", encoding="utf-8") as f:
                        history_data = json.load(f)
                except Exception as e:
                    print(f"âš  Failed to load history file: {e}")
            
            # åˆå¹¶æ–°æ•°æ®
            existing_links = {item['link'] for item in history_data}
            added_count = 0
            # ç¨å¾®é€†åºæ’å…¥ï¼Œä¿æŒæœ€æ–°çš„åœ¨æœ€å‰ (ä½†æˆ‘ä»¬è¦ä¿æŒé«˜åˆ†åœ¨å‰ï¼Ÿ)
            # ç­–ç•¥ï¼šå†å²è®°å½•æŒ‰æ—¶é—´å€’åºã€‚æœ¬æ¬¡æ›´æ–°æŒ‰åˆ†æ•°æ’åºã€‚
            # ç®€å•è¿½åŠ ï¼š
            for art in reversed(all_new_articles): 
                if art['link'] not in existing_links:
                    history_data.insert(0, art)
                    added_count += 1
            
            if added_count > 0:
                with open(history_file, "w", encoding="utf-8") as f:
                    json.dump(history_data, f, ensure_ascii=False, indent=2)
                print(f"ğŸ“š Appended {added_count} articles to {history_file}")
            
        # æ‰“å°é¢„è§ˆ (åˆ†æ )
        print("\n" + "="*40)
        print("ğŸ“° INDUSTRY NEWS & UPDATES (Recommended)")
        print("="*40)
        for i, news in enumerate(news_list, 1):
            print(f"{i}. [Score:{news['score']}] {news['title']}")
            print(f"    â­ {news.get('score_reason', 'Base')}")
            print(f"   ğŸ“… {news.get('date', 'N/A')} | ğŸ¢ {news.get('venue', news.get('source_domain', ''))}")
            print(f"   ğŸ”— {news['link']}")
            print(f"   ğŸ‡¨ğŸ‡³ {news['summary']}")
            print("-" * 20)

        print("\n" + "="*40)
        print("ğŸ“œ ACADEMIC PAPERS & RESEARCH (Recommended)")
        print("="*40)
        for i, paper in enumerate(paper_list, 1):
            print(f"{i}. [Score:{paper['score']}] {paper['title']}")
            print(f"    â­ {paper.get('score_reason', 'Base')}")
            print(f"   ğŸ“… {paper.get('date', 'N/A')} | ğŸ› {paper.get('venue', 'Arxiv')}")
            print(f"   ğŸ”— {paper['link']}")
            print(f"   ğŸ‡¨ğŸ‡³ {paper['summary']}")
            print("-" * 20)
        print("")
            
    else:
        print(f"\nğŸ’¤ {result_message}")

    # æ— è®ºæœ‰æ— æ–°æ–‡ç« ï¼Œéƒ½è¦ä¿å­˜çŠ¶æ€ï¼ˆåŒ…æ‹¬ hashesï¼‰
    storage.save()
    print("âœ… History updated (including content hashes).")

    return result_message

# Cloud Function Entry Point
def run_scraper(request):
    """
    HTTP Cloud Function Entry Point
    Accepts a request object (flask.Request) and returns text.
    """
    print("ğŸš€ Cloud Function triggered!")
    
    # åœ¨ Python 3.7+ çš„ cloud function ç¯å¢ƒä¸­è¿è¡Œ async ä»£ç 
    try:
        if sys.platform == "win32":
            loop = asyncio.ProactorEventLoop()
            asyncio.set_event_loop(loop)
            result = loop.run_until_complete(monitor_news())
        else:
            result = asyncio.run(monitor_news())
            
        return f"âœ… Success: {result}"
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return f"âŒ Error: {str(e)}"

if __name__ == "__main__":
    # æœ¬åœ°ç›´æ¥è¿è¡Œ
    print(run_scraper(None))
