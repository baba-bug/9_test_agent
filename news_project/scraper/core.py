import json
import re
from httpx import AsyncClient
from openai import OpenAI
from typing import List, Dict, Any
import xml.etree.ElementTree as ET

from .utils import clean_html_for_ai
from .config import DEEPSEEK_API_KEY, DEEPSEEK_BASE_URL, SITE_COOKIES
from .rankings import get_ranking, CCF_RANKINGS # Import CCF_RANKINGS

from curl_cffi.requests import AsyncSession

# ... (fetch_webpage remains unchanged) ...



async def fetch_webpage(url: str) -> str:
    """
    Fetch webpage using curl_cffi to bypass bot detection (TLS fingerprinting).
    Handles TikTok's Remix JSON endpoint automatically.
    """
    # Special handling for TikTok: Use API endpoint to get JSON data
    if "newsroom.tiktok.com" in url and "_data" not in url:
        print(f"ğŸ”„ Switching to TikTok Data Endpoint for {url}")
        url = f"{url.split('?')[0]}?_data=routes%2F_app._index&lang=en"

    # Special handling for Arxiv (Use API for summaries)
    if "arxiv.org/list/" in url:
        try:
            print(f"ğŸ”„ Switching to Arxiv API for {url}...")
            # Extract category from URL: https://arxiv.org/list/cs.HC/recent -> cs.HC
            # or https://arxiv.org/list/cs.MA/recent
            import re
            match = re.search(r"list/([^/]+)", url)
            if match:
                category = match.group(1)
                # Query the last 15 papers to save tokens
                api_url = f"http://export.arxiv.org/api/query?search_query=cat:{category}&sortBy=submittedDate&sortOrder=descending&max_results=15"
                
                async with AsyncSession(impersonate="chrome120") as s:
                    response = await s.get(api_url)
                    
                    if response.status_code == 200:
                        root = ET.fromstring(response.content)
                        ns = {'atom': 'http://www.w3.org/2005/Atom', 'arxiv': 'http://arxiv.org/schemas/atom'}
                        
                        html_parts = [f"<html><body><h1>Arxiv {category} Recent Papers</h1>"]
                        
                        for entry in root.findall("atom:entry", ns):
                            title = entry.find("atom:title", ns).text.strip().replace("\n", " ")
                            summary = entry.find("atom:summary", ns).text.strip().replace("\n", " ")
                            link = entry.find("atom:id", ns).text.strip()
                            published = entry.find("atom:published", ns).text.strip()
                        
                        # Extract Venue Info
                            # Extract Venue Info
                            journal_ref = entry.find("arxiv:journal_ref", ns)
                            comment = entry.find("arxiv:comment", ns)
                            
                            venue_info = []
                            if journal_ref is not None:
                                venue_info.append(f"Journal: {journal_ref.text}")
                            if comment is not None:
                                venue_info.append(f"Comment: {comment.text}")
                            
                            venue_str = " | ".join(venue_info)
                            if venue_str:
                                # Use RAG/Lookup to get rating
                                venue_str = get_ranking(venue_str)
                            
                            html_parts.append(f"<article>")
                            html_parts.append(f"<h2>{title}</h2>")
                            html_parts.append(f"<p>Date: {published}</p>")
                            html_parts.append(f"<p>Venue: {venue_str}</p>")
                            html_parts.append(f"<a href='{link}'>Paper Link</a>")
                            html_parts.append(f"<div>{summary}</div>")
                            html_parts.append(f"</article><hr/>")
                            
                        html_parts.append("</body></html>")
                        return "".join(html_parts)
                    else:
                        print(f"âš  Arxiv API Failed: {response.status_code}")
        except Exception as e:
            print(f"âš  Arxiv Logic Error: {e}")

    # Mimic Chrome 120
    async with AsyncSession(impersonate="chrome120") as s:
        print(f"ğŸ“¡ Fetching {url} (curl_cffi)...")
        
        headers = {
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9",
            "Referer": "https://www.google.com/"
        }
        
        # è‡ªåŠ¨åŒ¹é… Cookie
        for domain, cookie in SITE_COOKIES.items():
            if domain in url and cookie:
                print(f"ğŸ”‘ Injecting Cookie for {domain}")
                headers["Cookie"] = cookie
                break
            
        try:
            response = await s.get(
                url, 
                timeout=30,
                headers=headers
            )
            response.raise_for_status()
            
            # If it's a Remix JSON response (TikTok), extract the HTML content
            if "application/json" in response.headers.get("content-type", ""):
                try:
                    data = response.json()
                    # Try to find mainArticle content in TikTok structure
                    if "mainArticle" in data:
                        print("ğŸ§© Parsed TikTok JSON structure.")
                        html_content = data["mainArticle"].get("content", "")
                        title = data["mainArticle"].get("title", "")
                        date = data["mainArticle"].get("publishedDate", "")
                        # Prepend title/date to help AI
                        return f"<h1>{title}</h1><p>Date: {date}</p><div>{html_content}</div>"
                except Exception as e:
                    print(f"âš  Failed to parse TikTok JSON: {e}")
                    return response.text # Fallback
            
            return response.text
            
        except Exception as e:
            print(f"âŒ Fetch Error: {e}")
            return ""

async def extract_news_with_ai(html: str, url: str, mode: str = "news") -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨ AI æ™ºèƒ½æå–ä¿¡æ¯
    mode: "news" (é»˜è®¤æ–°é—») æˆ– "paper" (ç§‘ç ”è®ºæ–‡)
    """
    client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL)
    
    cleaned_text = clean_html_for_ai(html, url)
    
    if not cleaned_text:
        return []

    print(f"ğŸ“ Processing {url} as [{mode.upper()}] (Content len: {len(cleaned_text)})")
    
    # æ„é€  CCF ä¸Šä¸‹æ–‡ç®€è¡¨ (å‡å°‘ tokenï¼Œåªåˆ—å‡º A ç±»å’Œå¸¸è§ B ç±»)
    ccf_context = "CCF/Top Venue Reference (Class A=10, B=5, C=2):\n"
    top_venues = [k for k, v in CCF_RANKINGS.items() if v == "CCF A"][:30] # Top 30 A-class
    ccf_context += ", ".join(top_venues)
    
    # --- PROMPT DESIGN ---
    if mode == "paper":
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªé¡¶å°–ç§‘ç ”è®ºæ–‡é‰´èµä¸“å®¶ã€‚è¯·ä»ç½‘é¡µæ–‡æœ¬ä¸­æå–è®ºæ–‡åˆ—è¡¨ï¼Œå¹¶è¿›è¡Œæ·±åº¦è¯„åˆ†ã€‚
ä»»åŠ¡è¦æ±‚ï¼š
1. æå–è®ºæ–‡ä¿¡æ¯ï¼š
   - æ ‡é¢˜ (title): è‹±æ–‡åŸé¢˜
   - é“¾æ¥ (link): å¿…é¡»æ˜¯ç»å¯¹è·¯å¾„ã€‚
   - æ‘˜è¦ (summary): **ä¸­æ–‡æ€»ç»“**ï¼Œä¾§é‡ç ”ç©¶æ–¹æ³•ã€è´¡çŒ®å’Œåˆ›æ–°ç‚¹ã€‚
   - æ—¥æœŸ (date): å‘è¡¨æˆ–ä¸Šä¼ æ—¥æœŸã€‚
   - å‘è¡¨å¤„ (venue): æœŸåˆŠ/ä¼šè®®åç§°ã€‚
2. **æ·±åº¦è¯„åˆ† (Scoring)**ï¼š
   - `ai_score` (0-100): è¯­ä¹‰ç›¸å…³æ€§æ‰“åˆ†ã€‚ç”¨æˆ·å…´è¶£ç‚¹ï¼š**AI, Agent, HCI, XR/Spatial, Generation**. ç›¸å…³åº¦è¶Šé«˜åˆ†æ•°è¶Šé«˜ã€‚
   - `impact_score` (0-10): å­¦æœ¯å½±å“åŠ›ã€‚å‘è¡¨åœ¨ CCF A (å¦‚ CVPR, CHI, NeurIPS) æˆ– Top Journal (Nature/Science) å¾— 10 åˆ†ï¼›CCF B å¾— 5 åˆ†ï¼›ä¸€èˆ¬ä¼šè®® 2 åˆ†ï¼›Arxiv é¢„å°æœ¬ 1 åˆ†ã€‚
   - `is_tech_release` (bool): è®ºæ–‡æ˜¯å¦ä¼´éšä»£ç å‘å¸ƒ(GitHub)ã€æ¨¡å‹æƒé‡å‘å¸ƒ(HuggingFace)æˆ– Demo å‘å¸ƒã€‚
   - `score_reason` (str): ä¸€å¥è¯è§£é‡Šæ‰“åˆ†ç†ç”± (e.g., "Agenté¢†åŸŸCCF Aç±»è®ºæ–‡ï¼Œä¸”å¼€æºä»£ç ").
3. è¿‡æ»¤éè®ºæ–‡å†…å®¹ã€‚åªè¿”å› JSON æ•°ç»„ã€‚

å‚è€ƒï¼š
{ccf_context}

ç½‘é¡µå†…å®¹ï¼š
{cleaned_text[:50000]}

è¿”å› JSON æ ¼å¼ï¼š
[
    {{
        "title": "Paper Title",
        "link": "https://...",
        "summary": "ä¸­æ–‡æŠ€æœ¯æ€»ç»“...",
        "date": "2025-12-10",
        "venue": "CVPR 2025",
        "ai_score": 95,
        "impact_score": 10,
        "is_tech_release": true,
        "score_reason": "High interest Agent paper in CVPR with Code."
    }}
]
"""
    else: # mode == "news"
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªå‰æ²¿ç§‘æŠ€çŒæ‰‹ã€‚è¯·ä»ç½‘é¡µæ–‡æœ¬ä¸­æå–æ–°é—»åˆ—è¡¨ï¼Œå¹¶è¿›è¡Œä»·å€¼è¯„ä¼°ã€‚
ä»»åŠ¡è¦æ±‚ï¼š
1. æå–æ–°é—»ä¿¡æ¯ï¼š
   - æ ‡é¢˜ (title): è‹±æ–‡åŸé¢˜
   - é“¾æ¥ (link): å¿…é¡»æ˜¯ç»å¯¹è·¯å¾„ã€‚
   - æ‘˜è¦ (summary): **ä¸­æ–‡æ€»ç»“**ï¼Œä¾§é‡å‘ç”Ÿäº†ä»€ä¹ˆäº‹ã€äº§å“å‘å¸ƒæˆ–å•†ä¸šå½±å“ã€‚
   - æ—¥æœŸ (date): å…·ä½“æ—¥æœŸã€‚
   - æ¥æº (venue): æ–°é—»æ¥æºåç§°ã€‚
2. **æ·±åº¦è¯„åˆ† (Scoring)**ï¼š
   - `ai_score` (0-100): è¯­ä¹‰ç›¸å…³æ€§æ‰“åˆ†ã€‚ç”¨æˆ·å…´è¶£ç‚¹ï¼š**AI, Agent, HCI, XR/Spatial, Generation**.
   - `impact_score` (0-10): è¡Œä¸šå½±å“åŠ›ã€‚é‡ç£…äº§å“å‘å¸ƒ(GPT-5, Vision Pro 2) æˆ– é‡å¤§æŠ€æœ¯çªç ´(Sora) å¾— 10 åˆ†ï¼›æ™®é€šæ›´æ–° 3-5 åˆ†ã€‚
   - `is_tech_release` (bool): æ˜¯å¦æœ‰**å³åˆ»å¯ç”¨**çš„æŠ€æœ¯å‘å¸ƒ (Open Source, Model Weights, Public Beta)ã€‚
   - `score_reason` (str): ä¸€å¥è¯è§£é‡Šæ‰“åˆ†ç†ç”± (e.g., "é‡ç£…æ¨¡å‹ GPT-5 å‘å¸ƒ").
3. è¿‡æ»¤éæ–°é—»å†…å®¹ã€‚åªè¿”å› JSON æ•°ç»„ã€‚

ç½‘é¡µå†…å®¹ï¼š
{cleaned_text[:50000]}

è¿”å› JSON æ ¼å¼ï¼š
[
    {{
        "title": "News Title",
        "link": "https://...",
        "summary": "ä¸­æ–‡æ–°é—»æ‘˜è¦...",
        "date": "2025-12-10",
        "venue": "The Verge",
        "ai_score": 85,
        "impact_score": 10,
        "is_tech_release": true,
        "score_reason": "Major model release."
    }}
]
"""
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ–°é—»æå–ä¸“å®¶ã€‚åªè¿”å›çº¯å‡€çš„ JSON æ•°ç»„ã€‚summary å¿…é¡»æ˜¯ä¸­æ–‡ã€‚"},
                {"role": "user", "content": prompt}
            ],
            stream=False,
            temperature=0.1
        )
        
        result_text = response.choices[0].message.content.strip()
        
        # æ¸…ç† Markdown æ ‡è®°
        if result_text.startswith("```"):
            result_text = re.sub(r"^```(json)?|```$", "", result_text, flags=re.MULTILINE).strip()
            
        articles = json.loads(result_text)
        
        # åå¤„ç†å’ŒéªŒè¯
        valid_articles = []
        if isinstance(articles, list):
            for art in articles:
                # ç¡®ä¿æœ‰æ ‡é¢˜å’Œé“¾æ¥
                if art.get('title') and art.get('link'):
                    # è¡¥å…¨æ¥æº
                    art['source_domain'] = url.split('/')[2]
                    valid_articles.append(art)
                    
        return valid_articles

    except Exception as e:
        print(f"âŒ AI Extraction failed for {url}: {e}")
        return []
