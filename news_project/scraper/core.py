import json
import re
from httpx import AsyncClient
from openai import OpenAI
from typing import List, Dict, Any
import xml.etree.ElementTree as ET

from .utils import clean_html_for_ai
from .config import DEEPSEEK_API_KEY, DEEPSEEK_BASE_URL, SITE_COOKIES
from .rankings import get_ranking, CCF_RANKINGS, get_venue_score # Updated Import

from curl_cffi.requests import AsyncSession

# ... (fetch_webpage remains unchanged) ...



async def fetch_webpage(url: str) -> str:
    """
    Fetch webpage using curl_cffi to bypass bot detection (TLS fingerprinting).
    Handles TikTok's Remix JSON endpoint automatically.
    """
    # Special handling for TikTok: Use API endpoint to get JSON data
    if "newsroom.tiktok.com" in url and "_data" not in url:
        print(f"ğŸ”„ Switching to TikTok Data Endpoint for {url}")
        url = f"{url.split('?')[0]}?_data=routes%2F_app._index&lang=en"

    # Special handling for Arxiv (Use API for summaries)
    if "arxiv.org/list/" in url:
        try:
            print(f"ğŸ”„ Switching to Arxiv API for {url}...")
            # Extract category from URL: https://arxiv.org/list/cs.HC/recent -> cs.HC
            # or https://arxiv.org/list/cs.MA/recent
            import re
            # Extract category from URL: https://arxiv.org/list/cs.HC/recent -> cs.HC
            import re
            from datetime import datetime, timedelta, timezone
            
            match = re.search(r"list/([^/]+)", url)
            if match:
                category = match.group(1)
                
                # Pagination Logic: Fetch all papers from past 24 hours (or slightly more buffer)
                # Setting a safety cap of 100 papers or 5 pages to prevent infinite loops
                html_parts = [f"<html><body><h1>Arxiv {category} Recent Papers</h1>"]
                
                offset = 0
                max_results = 20 # Batch size per request
                fetched_count = 0
                cutoff_date = datetime.now(timezone.utc) - timedelta(hours=72) # 24h -> 3 days to cover weekends
                stop_fetching = False
                
                print(f"ğŸ”„ Arxiv Pagination: Fetching {category} papers since {cutoff_date.date()}...")

                async with AsyncSession(impersonate="chrome120") as s:
                    while not stop_fetching and fetched_count < 100:
                        api_url = f"http://export.arxiv.org/api/query?search_query=cat:{category}&sortBy=submittedDate&sortOrder=descending&start={offset}&max_results={max_results}"
                        
                        try:
                            response = await s.get(api_url)
                            if response.status_code != 200:
                                print(f"âš  Arxiv API Error {response.status_code} at offset {offset}")
                                break
                                
                            root = ET.fromstring(response.content)
                            ns = {'atom': 'http://www.w3.org/2005/Atom', 'arxiv': 'http://arxiv.org/schemas/atom'}
                            
                            entries = root.findall("atom:entry", ns)
                            if not entries:
                                break # No more results
                                
                            batch_valid_count = 0
                            for entry in entries:
                                # Date Check
                                published_str = entry.find("atom:published", ns).text.strip()
                                # Format: 2025-12-11T14:30:00Z
                                try:
                                    pub_date = datetime.strptime(published_str, "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=timezone.utc)
                                except:
                                    # Fallback if format fails
                                    pub_date = datetime.now(timezone.utc)

                                if pub_date < cutoff_date:
                                    stop_fetching = True
                                    # Don't break immediately if mixed sort, but Arxiv is sorted by Date.
                                    # However, "submittedDate" isn't strictly "published" date? 
                                    # Arxiv API: sortBy=submittedDate is reliable for new papers.
                                    break 
                                
                                batch_valid_count += 1
                                fetched_count += 1
                                
                                title = entry.find("atom:title", ns).text.strip().replace("\n", " ")
                                summary = entry.find("atom:summary", ns).text.strip().replace("\n", " ")
                                link = entry.find("atom:id", ns).text.strip()
                                
                                # Extract Venue Info
                                journal_ref = entry.find("arxiv:journal_ref", ns)
                                comment = entry.find("arxiv:comment", ns)
                                
                                venue_info = []
                                if journal_ref is not None:
                                    venue_info.append(f"Journal: {journal_ref.text}")
                                if comment is not None:
                                    venue_info.append(f"Comment: {comment.text}")
                                
                                venue_str = " | ".join(venue_info)
                                if venue_str:
                                    venue_str = get_ranking(venue_str)
                                
                                html_parts.append(f"<article>")
                                html_parts.append(f"<h2>{title}</h2>")
                                html_parts.append(f"<p>Date: {published_str}</p>")
                                html_parts.append(f"<p>Venue: {venue_str}</p>")
                                # Crucial: Expose Link as text because clean_html_for_ai strips tags/attributes
                                html_parts.append(f"<p>Link: {link}</p>") 
                                html_parts.append(f"<div>{summary}</div>")
                                html_parts.append(f"</article><hr/>")
                            
                            print(f"   ğŸ”¹ Batch {offset}: Found {batch_valid_count} recent papers (Total: {fetched_count})")
                            
                            if batch_valid_count < len(entries):
                                # If we filtered out some papers in this batch due to date, stop.
                                stop_fetching = True
                            
                            offset += max_results
                            
                        except Exception as e:
                            print(f"âš  Arxiv Loop Error: {e}")
                            break
                            
                html_parts.append("</body></html>")
                return "".join(html_parts)
                
        except Exception as e:
            print(f"âš  Arxiv Logic Error: {e}")

    # Mimic Chrome 120
    async with AsyncSession(impersonate="chrome120") as s:
        print(f"ğŸ“¡ Fetching {url} (curl_cffi)...")
        
        headers = {
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9",
            "Referer": "https://www.google.com/"
        }
        
        # è‡ªåŠ¨åŒ¹é… Cookie
        for domain, cookie in SITE_COOKIES.items():
            if domain in url and cookie:
                print(f"ğŸ”‘ Injecting Cookie for {domain}")
                headers["Cookie"] = cookie
                break
            
        try:
            response = await s.get(
                url, 
                timeout=30,
                headers=headers
            )
            response.raise_for_status()
            
            # If it's a Remix JSON response (TikTok), extract the HTML content
            if "application/json" in response.headers.get("content-type", ""):
                try:
                    data = response.json()
                    # Try to find mainArticle content in TikTok structure
                    if "mainArticle" in data:
                        print("ğŸ§© Parsed TikTok JSON structure.")
                        html_content = data["mainArticle"].get("content", "")
                        title = data["mainArticle"].get("title", "")
                        date = data["mainArticle"].get("publishedDate", "")
                        # Prepend title/date to help AI
                        return f"<h1>{title}</h1><p>Date: {date}</p><div>{html_content}</div>"
                except Exception as e:
                    print(f"âš  Failed to parse TikTok JSON: {e}")
                    return response.text # Fallback
            
            return response.text
            
        except Exception as e:
            print(f"âŒ Fetch Error: {e}")
            return ""

async def extract_news_with_ai(html: str, url: str, mode: str = "news") -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨ AI æ™ºèƒ½æå–ä¿¡æ¯
    mode: "news" (é»˜è®¤æ–°é—») æˆ– "paper" (ç§‘ç ”è®ºæ–‡)
    """
    client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL)
    
    client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL)
    
    # è·å–ä»Šå¤©æ—¥æœŸ
    from datetime import date
    today_str = date.today().strftime("%Y-%m-%d")

    # Helper function to query AI
    def _query_ai(text_content: str) -> List[Dict[str, Any]]:
        # æ„é€  CCF ä¸Šä¸‹æ–‡ç®€è¡¨
        ccf_context = "Rankings Reference:\n"
        class_a = [k for k, v in CCF_RANKINGS.items() if v == "CCF A"]
        class_b = [k for k, v in CCF_RANKINGS.items() if v == "CCF B"]
        class_c = [k for k, v in CCF_RANKINGS.items() if v == "CCF C"]
        
        ccf_context += f"Class A (Top): {', '.join(class_a[:50])}\n"
        ccf_context += f"Class B (Excellent): {', '.join(class_b[:30])}\n"
        
        # --- PROMPT DESIGN ---
        if mode == "paper":
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªé¡¶å°–ç§‘ç ”è®ºæ–‡é‰´èµä¸“å®¶ã€‚è¯·ä»ç½‘é¡µæ–‡æœ¬ä¸­æå–è®ºæ–‡åˆ—è¡¨ï¼Œå¹¶è¿›è¡Œæ·±åº¦è¯„åˆ†ã€‚
ä»»åŠ¡è¦æ±‚ï¼š
1. æå–è®ºæ–‡ä¿¡æ¯ï¼š
   - æ ‡é¢˜ (title): è‹±æ–‡åŸé¢˜
   - é“¾æ¥ (link): å¿…é¡»æ˜¯ç»å¯¹è·¯å¾„ã€‚
   - æ‘˜è¦ (summary): **ä¸­æ–‡æ€»ç»“**ï¼Œä¾§é‡ç ”ç©¶æ–¹æ³•ã€è´¡çŒ®å’Œåˆ›æ–°ç‚¹ã€‚
   - æ—¥æœŸ (date): æ ¼å¼ç»Ÿä¸€ä¸º `YYYY-MM-DD`. å¦‚æœæ–‡ä¸­æ—¥æœŸä¸æ˜ç¡®ï¼Œé»˜è®¤ä¸ºä»Šå¤© ({today_str}).
   - å‘è¡¨å¤„ (venue): æœŸåˆŠ/ä¼šè®®åç§°ã€‚
2. **æ·±åº¦è¯„åˆ† (Scoring)** for Impact:
   - `ai_score` (0-100): è¯­ä¹‰ç›¸å…³æ€§æ‰“åˆ†ã€‚ç”¨æˆ·å…´è¶£ç‚¹ï¼š**AI, Agent, HCI, XR/Spatial, Generation, Diffusion, 3D, VR, AR, MR, Spatial Computing, Brain, Recognition, Cognitive, Health, Sense Control, Emotion, Affective, Eye Tracking, Gesture, Face**. ç›¸å…³åº¦è¶Šé«˜åˆ†æ•°è¶Šé«˜ã€‚
   - `impact_score` (0-50): å­¦æœ¯å½±å“åŠ›ã€‚å‘è¡¨åœ¨ CCF A (å¦‚ CVPR, CHI, NeurIPS) æˆ– Top Journal (Nature/Science) å¾— 50 åˆ†ï¼›CCF B å¾— 25 åˆ†ï¼›ä¸€èˆ¬ä¼šè®® 10 åˆ†ï¼›Arxiv é¢„å°æœ¬ 5 åˆ†ã€‚
   - `is_tech_release` (bool): è®ºæ–‡æ˜¯å¦ä¼´éšä»£ç å‘å¸ƒ(GitHub)ã€æ¨¡å‹æƒé‡å‘å¸ƒ(HuggingFace)æˆ– Demo å‘å¸ƒã€‚
   - `code_url` (str): å¦‚æœ `is_tech_release` ä¸ºçœŸï¼Œæå–å…·ä½“çš„å¼€æºé“¾æ¥ (GitHub/HuggingFace), å¦åˆ™ä¸º null.
   - `score_reason` (str): ä¸€å¥è¯è§£é‡Šæ‰“åˆ†ç†ç”± (e.g., "Agenté¢†åŸŸCCF Aç±»è®ºæ–‡ï¼Œä¸”å¼€æºä»£ç ").
3. è¿‡æ»¤éè®ºæ–‡å†…å®¹ã€‚åªè¿”å› JSON æ•°ç»„ã€‚

å‚è€ƒï¼š
{ccf_context}

ç½‘é¡µå†…å®¹ï¼š
{text_content[:60000]}

è¿”å› JSON æ ¼å¼ï¼š
[
    {{
        "title": "Paper Title",
        "link": "https://...",
        "summary": "ä¸­æ–‡æŠ€æœ¯æ€»ç»“...",
        "date": "2025-12-10",
        "venue": "CVPR 2025",
        "ai_score": 95,
        "impact_score": 50,
        "is_tech_release": true,
        "code_url": "https://github.com/...",
        "score_reason": "High interest Agent paper in CVPR with Code."
    }}
]
"""
        else: # mode == "news"
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªå‰æ²¿ç§‘æŠ€çŒæ‰‹ã€‚è¯·ä»ç½‘é¡µæ–‡æœ¬ä¸­æå–æ–°é—»åˆ—è¡¨ï¼Œå¹¶è¿›è¡Œä»·å€¼è¯„ä¼°ã€‚
ä»»åŠ¡è¦æ±‚ï¼š
1. æå–æ–°é—»ä¿¡æ¯ï¼š
   - æ ‡é¢˜ (title): è‹±æ–‡åŸé¢˜
   - é“¾æ¥ (link): å¿…é¡»æ˜¯ç»å¯¹è·¯å¾„ã€‚
   - æ‘˜è¦ (summary): **ä¸­æ–‡æ€»ç»“**ï¼Œä¾§é‡å‘ç”Ÿäº†ä»€ä¹ˆäº‹ã€äº§å“å‘å¸ƒæˆ–å•†ä¸šå½±å“ã€‚
   - æ—¥æœŸ (date): æ ¼å¼ç»Ÿä¸€ä¸º `YYYY-MM-DD`. å¦‚æœæ–‡ä¸­æ—¥æœŸä¸æ˜ç¡®ï¼Œé»˜è®¤ä¸ºä»Šå¤© ({today_str}).
   - æ¥æº (venue): æ–°é—»æ¥æºåç§°ã€‚
2. **æ·±åº¦è¯„åˆ† (Scoring)**ï¼š
   - `ai_score` (0-100): è¯­ä¹‰ç›¸å…³æ€§æ‰“åˆ†ã€‚ç”¨æˆ·å…´è¶£ç‚¹ï¼š**AI, Agent, HCI, XR/Spatial, Generation, Diffusion, 3D, VR, AR, MR, Spatial Computing, Brain, Recognition, Cognitive, Health, Sense, Control, Emotion, Affective, Eye Tracking, Gesture, Face, Disability,**.
   - `impact_score` (0-50): è¡Œä¸šå½±å“åŠ›ã€‚é‡ç£…å¯ç©¿æˆ´äº§å“å‘å¸ƒ(GPT-5, Vision Pro 2) æˆ– é‡å¤§æŠ€æœ¯çªç ´(Sora) å¾— 50 åˆ†ï¼›æ™®é€šæ›´æ–° 10-20 åˆ†ã€‚
   - `is_tech_release` (bool): æ˜¯å¦æœ‰**å³åˆ»å¯ç”¨**çš„æŠ€æœ¯å‘å¸ƒ (Open Source, Model Weights, Public Beta)ã€‚
   - `code_url` (str): å¦‚æœ `is_tech_release` ä¸ºçœŸï¼Œæå–å…·ä½“çš„å¼€æºé“¾æ¥ (GitHub/HuggingFace), å¦åˆ™ä¸º null.
   - `score_reason` (str): ä¸€å¥è¯è§£é‡Šæ‰“åˆ†ç†ç”± (e.g., "é‡ç£…æ¨¡å‹ GPT-5 å‘å¸ƒ").
   - 'negtive score'(-100-0): ä¸å…³å¿ƒç›‘ç®¡æ”¿ç­–ã€æ³•å¾‹è¿˜æœ‰CPUå’Œæ˜¾å¡çš„åŸºç¡€è®¾æ–½ç¡¬ä»¶æ¶ˆæ¯, å‡ºç°ç»™è´Ÿåˆ†ã€‚
3. è¿‡æ»¤éæ–°é—»å†…å®¹ã€‚åªè¿”å› JSON æ•°ç»„ã€‚

ç½‘é¡µå†…å®¹ï¼š
{text_content[:60000]}

è¿”å› JSON æ ¼å¼ï¼š
[
    {{
        "title": "News Title",
        "link": "https://...",
        "summary": "ä¸­æ–‡æ–°é—»æ‘˜è¦...",
        "date": "2025-12-10",
        "venue": "The Verge",
        "ai_score": 85,
        "impact_score": 50,
        "is_tech_release": true,
        "score_reason": "Major model release."
    }}
]
"""
        try:
            response = client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ–°é—»æå–ä¸“å®¶ã€‚åªè¿”å›çº¯å‡€çš„ JSON æ•°ç»„ã€‚summary å¿…é¡»æ˜¯ä¸­æ–‡ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                stream=False,
                temperature=0.1
            )
            
            result_text = response.choices[0].message.content.strip()
            if result_text.startswith("```"):
                result_text = re.sub(r"^```(json)?|```$", "", result_text, flags=re.MULTILINE).strip()
            
            # DEBUG
            print(f"ğŸ” DEBUG AI RESULT: {result_text[:500]}")
                
            data = json.loads(result_text)
            return data if isinstance(data, list) else []
            
        except Exception as e:
            print(f"âŒ AI Extraction Inner Error: {e}")
            print(f"ğŸ” DEBUG ERROR OUTPUT: {result_text if 'result_text' in locals() else 'N/A'}")
            return []

    # --- MAIN EXTRACTION LOGIC ---
    
    final_articles = []
    
    # Check for Arxiv Batching Strategy
    if mode == "paper" and "arxiv.org" in url and "<h1>Arxiv" in html:
        print("ğŸ“¦ Batching Arxiv papers for AI extraction...")
        # Split raw HTML by article tag
        raw_articles = re.findall(r"<article>.*?</article>", html, re.DOTALL)
        
        # Batch size of 8 is safe for DeepSeek output limits (4k tokens)
        batch_size = 8
        
        for i in range(0, len(raw_articles), batch_size):
            batch = raw_articles[i : i+batch_size]
            print(f"   ğŸ¤– AI Processing Batch {i//batch_size + 1} ({len(batch)} items)...")
            
            # Clean just this batch
            batch_text = "<html><body>" + "\n".join(batch) + "</body></html>"
            cleaned_batch = clean_html_for_ai(batch_text, url)
            
            if cleaned_batch:
                # print(f"   ğŸ“„ Cleaned Batch Len: {len(cleaned_batch)}")
                batch_results = _query_ai(cleaned_batch)
                final_articles.extend(batch_results)
                
    else:
        # Standard Single-Pass Logic
        cleaned_text = clean_html_for_ai(html, url)
        if cleaned_text:
            print(f"ğŸ“ Processing {url} as [{mode.upper()}] (Content len: {len(cleaned_text)})")
            final_articles = _query_ai(cleaned_text)

    # --- POST PROCESSING & VALIDATION ---
    valid_articles = []
    
    for art in final_articles:
        # ç¡®ä¿æœ‰æ ‡é¢˜å’Œé“¾æ¥
        if art.get('title') and art.get('link'):
            
            # STRICT CHECK: Only for Arxiv papers
            # If Arxiv paper claims release but has no code_url, reset is_tech_release
            if mode == "paper" and "arxiv.org" in url:
                code_url = art.get('code_url')
                if art.get('is_tech_release') and (not code_url or str(code_url).lower() in ["none", "null", ""]):
                    # print(f"âš ï¸ Strict Check (Arxiv): '{art['title']}' claimed release but no URL. Resetting to False.")
                    art['is_tech_release'] = False
                    art['code_url'] = None
                    if "score_reason" in art:
                        art['score_reason'] += " (Arxiv: Code link missing, boost removed)"
            
            # è¡¥å…¨æ¥æº
            art['source_domain'] = url.split('/')[2]
            
            # RE-CALCULATE IMPACT SCORE using Python Logic (Authoritative)
            # Trust the hardcoded list/IF values over LLM's guess if venue is recognized
            raw_venue = art.get('venue', '')
            python_score = get_venue_score(raw_venue)
            try:
                ai_score_val = int(art.get('impact_score', 0))
            except:
                ai_score_val = 0
            
            if python_score > ai_score_val:
                print(f"ğŸ“ˆ Boosting Impact Score for '{art['title']}': {ai_score_val} -> {python_score} (Venue: {raw_venue})")
                art['impact_score'] = python_score
                if "score_reason" in art:
                    art['score_reason'] += f" [Impact Boosted by Verified Venue/IF: {raw_venue}]"

            valid_articles.append(art)
            
    return valid_articles
